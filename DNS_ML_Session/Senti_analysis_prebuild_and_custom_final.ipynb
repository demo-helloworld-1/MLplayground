{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/demo-helloworld-1/MLplayground/blob/main/Sentianalysiswithcsv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1U7UUAphLZi",
    "outputId": "1b8a3a15-c8bf-472e-853a-e5303577768f"
   },
   "outputs": [],
   "source": [
    "!pip install nltk textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRSi7fmNhMJH"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YlI21o8IhMs3",
    "outputId": "a6291d14-e01b-46f5-ca3a-ccc69d4bbb27"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJ4W82IOhN_3",
    "outputId": "0f728f54-8989-434e-bf6c-87571852b744"
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    # Classify sentiment as positive, negative, or neutral\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'\n",
    "# Example usage:\n",
    "text = \"I need to buy this product again and again, but this product could be much better\"\n",
    "sentiment = analyze_sentiment(text)\n",
    "print(f\"Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNDY1KkJhuPU",
    "outputId": "59bf84ef-a479-4acf-cace-ae67ef9dd496"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    # Calculate sentiment percentages\n",
    "    positive_percentage = (analysis.sentiment.polarity + 1) / 2 * 100\n",
    "    negative_percentage = (1 - analysis.sentiment.polarity) / 2 * 100\n",
    "\n",
    "    # Classify sentiment based on percentages\n",
    "    if positive_percentage > negative_percentage:\n",
    "        sentiment = 'positive'\n",
    "    elif positive_percentage < negative_percentage:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    "\n",
    "    # Return sentiment and percentages\n",
    "    return sentiment, positive_percentage, negative_percentage\n",
    "\n",
    "# Example usage\n",
    "text = \"         \"\n",
    "sentiment, positive_percentage, negative_percentage = analyze_sentiment(text)\n",
    "\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Positive Percentage: {positive_percentage:.2f}%\")\n",
    "print(f\"Negative Percentage: {negative_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hr-vQ1Kk9dj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"abhi8923shriv/sentiment-analysis-dataset\")\n",
    "try:\n",
    "       df = pd.read_csv(path+'/train.csv', encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "       df = pd.read_csv(path+'/train.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSazr5G1uZKf"
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment_file(text):\n",
    "    # Convert the input to string before processing\n",
    "    text = str(text)\n",
    "    analysis = TextBlob(text)\n",
    "    # Classify sentiment as positive, negative, or neutral\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjP2DFnQlK9I"
   },
   "outputs": [],
   "source": [
    "df['sentiment'] = df['text'].apply(analyze_sentiment_file)  # Assuming your feedback column is named 'feedback'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqYcNIoulPFo",
    "outputId": "7d0f03c1-a2f8-40ec-fdd8-d66eef6630d8"
   },
   "outputs": [],
   "source": [
    "print(df[['text', 'sentiment']].head(20))  # Print the relevant columns\n",
    "# df.to_excel('output_file.xlsx', index=False)  # Save results to a new Excel file (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clFQUhtOms4N"
   },
   "source": [
    "# Lets Begin with Senti Analysis on one of the Twitter's Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKLHiED1m2aw"
   },
   "source": [
    "Credits: Kaggle Open Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKLHiED1m2aw"
   },
   "source": [
    "1. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Alternatively, you can fill missing values with a specific value\n",
    "# df.fillna(\"missing\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets Cleanz the values, this handles the tags, URLs and Stopping Words(\"the, is, in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)  # Remove mentions and hashtags\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.split()\n",
    "    text = [word for word in text if not word in set(stopwords.words('english'))]  # Remove stopwords\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to the DataFrame\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['text','cleaned_text']].head(20))  # Print the relevant columns\n",
    "# df.to_excel('output_file.xlsx', index=False)  # Save results to a new Excel file (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming/Lemmatization\n",
    "\n",
    "Stemming reduces words to their base or root form. For example:\n",
    "\n",
    "\"running\" -> \"run\" |\n",
    "\"runner\" -> \"run\" |\n",
    "\"runs\" -> \"run\" \n",
    "\n",
    "Lemmatization reduces words to their root form, but it considers the context and transforms words into their meaningful base forms. For example:\n",
    "\n",
    "\"running\" -> \"run\" |\n",
    "\"better\" -> \"good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to stem/lemmatize text\n",
    "def stem_lemmatize_text(text):\n",
    "    text = text.split()\n",
    "    text = [stemmer.stem(word) for word in text]  # Stemming\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]  # Lemmatization\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "# Apply the stem_lemmatize_text function to the cleaned text\n",
    "df['processed_text'] = df['cleaned_text'].apply(stem_lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['cleaned_text','processed_text']].head(20))  # Print the relevant columns\n",
    "# df.to_excel('output_file.xlsx', index=False)  # Save results to a new Excel file (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count distribution\n",
    "df['word_count'] = df['processed_text'].apply(lambda x: len(x.split()))\n",
    "print(df['word_count'].describe())\n",
    "\n",
    "# Sentence length distribution\n",
    "df['sentence_length'] = df['processed_text'].apply(lambda x: len(x))\n",
    "print(df['sentence_length'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides summary statistics for the word count in your dataset:\n",
    "\n",
    "count: The number of text entries in your dataset.\n",
    "\n",
    "mean: The average number of words per text entry.\n",
    "\n",
    "std: The standard deviation, which tells you how much the word counts vary from the mean.\n",
    "\n",
    "min: The minimum word count in your dataset.\n",
    "\n",
    "25% (1st quartile): The word count at the 25th percentile.\n",
    "\n",
    "50% (median): The word count at the 50th percentile (middle value).\n",
    "\n",
    "75% (3rd quartile): The word count at the 75th percentile.\n",
    "\n",
    "max: The maximum word count in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides summary statistics for the sentence length in your dataset:\n",
    "\n",
    "count: The number of text entries in your dataset.\n",
    "\n",
    "mean: The average number of characters per text entry.\n",
    "\n",
    "std: The standard deviation, which tells you how much the sentence lengths vary from the mean.\n",
    "\n",
    "min: The minimum sentence length in your dataset.\n",
    "\n",
    "25% (1st quartile): The sentence length at the 25th percentile.\n",
    "\n",
    "50% (median): The sentence length at the 50th percentile (middle value).\n",
    "\n",
    "75% (3rd quartile): The sentence length at the 75th percentile.\n",
    "\n",
    "max: The maximum sentence length in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(df['processed_text']))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for sentiment class frequency\n",
    "df['sentiment'].value_counts().plot(kind='bar')\n",
    "plt.title('Sentiment Class Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize Count Vectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the processed text\n",
    "X = count_vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "# Check the shape of the resulting matrix\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Classification Report:\\n{report}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to a file\n",
    "with open('sentiment_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the model from the file\n",
    "with open('sentiment_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example new text data\n",
    "new_texts = [\"I was really looking forward to this event, but after attending, I found it to be an utter disappointment due to the poor organization, rude staff, and lack of promised activities, although I met some wonderful people.\"]\n",
    "\n",
    "# Transform the new text data using the same Count Vectorizer\n",
    "new_texts_transformed = count_vectorizer.transform(new_texts)\n",
    "\n",
    "# Predict sentiments\n",
    "new_predictions = loaded_model.predict(new_texts_transformed)\n",
    "\n",
    "print(new_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yes, we are Done with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
